{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBS1QLtvwzSe"
      },
      "source": [
        "# Crawling PTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsQNAdaSw1qh",
        "outputId": "2db3a720-1509-47cd-cbf0-b673f537f0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=9f65aa1983c5112944b6ea7c3a51e084a787fec39f36cb7f2d44a0961b4b3bd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install requests\n",
        "!pip install html5lib\n",
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XxbtMhkW2SQR"
      },
      "outputs": [],
      "source": [
        "# constant\n",
        "LAST_INDEX = -1\n",
        "FIRST_INDEX = 1\n",
        "INCREMENT_BY_ONE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "avu3r08fxL-B"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def request_url(url):\n",
        "    return requests.get(url)\n",
        "\n",
        "def request_header_url(header, url_website):\n",
        "    headers = {'User-Agent': header}\n",
        "    return requests.get(url=url_website, headers=headers)\n",
        "\n",
        "def parse_website(request):\n",
        "    \"\"\"Use html5lib library to parse\"\"\"\n",
        "    return BeautifulSoup(request.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
        "\n",
        "def prettify_web_structure(parsed_page):\n",
        "    parsed_page.prettify()\n",
        "\n",
        "def get_content_table(web_element, tag, attributes):\n",
        "    return web_element.find(tag, attrs = attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmZ3VF-PxQVc"
      },
      "outputs": [],
      "source": [
        "# max_pages = 172 # total halaman di website PTA, terbaru februari 2023\n",
        "\n",
        "r = request_header_url(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1\", \"https://pta.trunojoyo.ac.id/c_search/byprod/10\")\n",
        "soup = parse_website(r)\n",
        "prettify_web_structure(soup)\n",
        "table = get_content_table(soup, \"div\", {\"id\":\"wrapper\"})\n",
        "\n",
        "pagination = table.findAll(\"a\", attrs = {\"class\":\"pag_button\"})\n",
        "total_pages = int(pagination[LAST_INDEX][\"href\"].split(\"/\")[LAST_INDEX])\n",
        "\n",
        "papers = []\n",
        "for pages in range(FIRST_INDEX, total_pages + INCREMENT_BY_ONE):\n",
        "\n",
        "    url_link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10\"\n",
        "    id_prodi = url_link.split(\"/\")[LAST_INDEX]\n",
        "    nama_prodi = \"\"\n",
        "    if (id_prodi == \"10\"):\n",
        "        nama_prodi = \"Teknik Informatika\"\n",
        "\n",
        "    r_pages = request_header_url(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\", f\"{url_link}/{pages}\")\n",
        "    soup_pages = parse_website(r_pages)\n",
        "    prettify_web_structure(soup_pages)\n",
        "    table_pages = get_content_table(soup_pages, \"div\", {\"id\":\"wrapper\"})\n",
        "\n",
        "    for article_row in table_pages.findAll(\"a\", attrs = {\"class\":\"gray button\"}):\n",
        "        r_article_row = request_header_url(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\", article_row[\"href\"])\n",
        "        soup_article_row = parse_website(r_article_row)\n",
        "        prettify_web_structure(soup_article_row)\n",
        "        table_article_row = get_content_table(soup_article_row, \"li\", {\"data-id\":\"id-1\", \"data-cat\":\"#luxury\"})\n",
        "        article_row_title = table_article_row.find('a', attrs = {\"class\":\"title\", \"href\":\"#\"}).string\n",
        "        article_row_penulis = \"\"\n",
        "        article_row_pembimbing1 = \"\"\n",
        "        article_row_pembimbing2 = \"\"\n",
        "        article_row_abstract = table_article_row.find('p', attrs = {\"align\":\"justify\"}).string\n",
        "        spans = table_article_row.find_all('span')\n",
        "        for span in spans:\n",
        "          span_text = span.get_text(strip=True)\n",
        "          if \"Penulis :\" in span_text:\n",
        "            article_row_penulis = span_text.replace(\"Penulis :\", \"\").strip()\n",
        "          elif \"Dosen Pembimbing I :\" in span_text:\n",
        "            article_row_pembimbing1 = span_text.replace(\"Dosen Pembimbing I :\", \"\").strip()\n",
        "          elif \"Dosen Pembimbing II :\" in span_text:\n",
        "            article_row_pembimbing2 = span_text.replace(\"Dosen Pembimbing II :\", \"\").strip()\n",
        "\n",
        "        papers.append([\n",
        "                article_row_title,\n",
        "                article_row_penulis,\n",
        "                article_row_pembimbing1,\n",
        "                article_row_pembimbing2,\n",
        "                article_row_abstract\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o6BJ507txUrg"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(papers, columns=[\"Judul\", \"Nama Penulis\", \"pembimbing I\", \"pembimbing II\", \"Abstrak\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UTybAWfG2kHY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "df.to_csv('/content/PTA.csv')\n",
        "# pd.read_csv('/content/drive/MyDrive/prosaindata/tugas/PTA.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}